{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先请认真学习百度python编码规范！！确认理解并认同规范，然后再写码。  \n",
    "评委抽空做一次code review不容易，请珍惜！别犯低级的编码规范问题。因为评委下次再review你的代码不知道是什么时候。  \n",
    "百度python编码规范  \n",
    "http://wiki.baidu.com/pages/viewpage.action?pageId=728793774  \n",
    "\n",
    "特别需要注意:  \n",
    "1、一审和二审评审过程中代码都不要自己+2合入  \n",
    "2、评审评审后，申请者需要在一定时间内修复（预计是2周以内）并反馈给评委，如果评委两周没有  \n",
    "回复也可单独联系评审，或者考试组织人。  \n",
    "3、建议在二级目录baidu/goodcoder 下新建代码模块，不要使用personal-code二级目录，因为personal-code没有自动的规范检测。  \n",
    "\n",
    "你确定完全理解《百度python编码规范》里的所有条款了？  \n",
    "\n",
    "考题：  \n",
    "\n",
    "迷你定向网页抓取器  \n",
    "在调研过程中，经常需要对一些网站进行定向抓取。由于python包含各种强大的库，使用python做定向抓取比较简单。请使用python开发一个迷你定向抓取器mini_spider.py，实现对种子链接的广度优先抓取，并把URL长相符合特定pattern的网页内容（图片或者html等）保存到磁盘上。\n",
    "\n",
    "程序运行：\n",
    "\n",
    "python mini_spider.py -c spider.conf   \n",
    "配置文件spider.conf：  \n",
    "\n",
    "[spider]   \n",
    "url_list_file: ./urls ; 种子文件路径   \n",
    "output_directory: ./output ; 抓取结果存储目录   \n",
    "max_depth: 1 ; 最大抓取深度(种子为0级)   \n",
    "crawl_interval: 1 ; 抓取间隔. 单位：秒   \n",
    "crawl_timeout: 1 ; 抓取超时. 单位：秒   \n",
    "target_url: .*\\.(html|gif|png|jpg|bmp)$ ; 需要存储的目标网页URL pattern(正则表达式) ,需要考虑兼容抓取html等情况，不止是抓取图片  \n",
    "thread_count: 8 ; 抓取线程数   \n",
    "\n",
    "种子文件每行一条链接，例如：  \n",
    "http://cup.baidu.com/spider/  \n",
    "http://www.baidu.com   \n",
    "http://www.sina.com.cn   \n",
    "  \n",
    "要求和注意事项：  \n",
    "0、严禁抄袭，一旦被发现，会被取消申请资格，并在部门、甚至公司全员通报！！  \n",
    "1、需要支持命令行参数处理。具体包含：-h(帮助)、-v(版本)、-c(配置文件)。  \n",
    "2、需要按照广度优先的顺序抓取网页,避免url的多次抓取。  \n",
    "3、单个网页**抓取**或**解析**失败，不能导致整个程序退出。**需要在日志中记录下错误原因并继续**。\\\n",
    "4、当程序完成所有抓取任务后，必须优雅退出。  \n",
    "5、从HTML提取链接时需要处理相对路径和绝对路径。  \n",
    "6、需要能够处理不同字符编码的网页(例如utf-8或gbk)。  \n",
    "7、网页存储时每个网页单独存为一个文件，以URL为文件名。**注意对URL中的特殊字符，需要做转义**。\\\n",
    "8、**要求支持多线程并行抓取**。\\\n",
    "9、代码严格遵守百度python编码规范。百度python编码规范：http://wiki.baidu.com/pages/viewpage.action?pageId=728793774  \n",
    "10、代码的可读性和可维护性好。注意模块、类、函数的设计和划分。  \n",
    "11、完成相应的单元测试和使用demo。你的demo必须可运行，单元测试有效而且通过。  \n",
    "12、创建readme.txt文件，写清楚代码布局、如何执行（包括依赖库如何安装等环境初始化工作）  \n",
    "  \n",
    "参考资料：  \n",
    "Python Good Coder考试指南：http://wiki.baidu.com/pages/viewpage.action?pageId=328311684  \n",
    "\n",
    "提示(下面的python库可能对你完成测试题有帮助)：  \n",
    "re(正则表达式)  \n",
    "参考：http://docs.python.org/2/library/re.html  \n",
    "参考：http://www.cnblogs.com/huxi/archive/2010/07/04/1771073.html  \n",
    "参考：http://blog.csdn.net/jgood/article/details/4277902  \n",
    "gevent/threading(多线程)  \n",
    "参考：http://docs.python.org/2/library/threading.html  \n",
    "参考：http://www.cnblogs.com/huxi/archive/2010/06/26/1765808.html  \n",
    "docopt/getopt/argparse(命令行参数处理)  \n",
    "参考：https://github.com/docopt/docopt  \n",
    "参考：http://docs.python.org/2/library/getopt.html  \n",
    "参考：http://andylin02.iteye.com/blog/845355  \n",
    "参考：http://docs.python.org/2/howto/argparse.html  \n",
    "参考：http://www.cnblogs.com/jianboqi/archive/2013/01/10/2854726.html  \n",
    "ConfigParser(配置文件读取)  \n",
    "参考：http://docs.python.org/2/library/configparser.html  \n",
    "参考：http://blog.chinaunix.net/uid-25890465-id-3312861.html  \n",
    "urllib/urllib2/httplib(网页下载)  \n",
    "参考：http://docs.python.org/2/library/urllib2.html  \n",
    "参考：http://blog.csdn.net/wklken/article/details/7364328  \n",
    "参考：http://www.nowamagic.net/academy/detail/1302872  \n",
    "pyquery/beautifulsoup4/HTMLParser/SGMLParser(HTML解析)  \n",
    "参考：http://docs.python.org/2/library/htmlparser.html  \n",
    "参考：http://cloudaice.com/yong-pythonde-htmlparserfen-xi-htmlye-mian/  \n",
    "参考：http://docs.python.org/2/library/sgmllib.html  \n",
    "参考：http://pako.iteye.com/blog/592009  \n",
    "urlparse(URL解析处理)  \n",
    "参考：http://docs.python.org/2/library/urlparse.html  \n",
    "参考：http://blog.sina.com.cn/s/blog_5ff7f94f0100qr3c.html  \n",
    "logging(日志处理)  \n",
    "参考：http://docs.python.org/2/library/logging.html  \n",
    "参考：http://kenby.iteye.com/blog/1162698  \n",
    "参考：http://my.oschina.net/leejun2005/blog/126713  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Parse options and execute commands.\"\"\"\n",
    "    try:\n",
    "        argparser = create_argparser()\n",
    "        args = argparser.parse_args()\n",
    "        if args.command is None:\n",
    "            # Python 3.3.1 under linux allows an empty command somehow\n",
    "            argparser.error(\"too few arguments\")\n",
    "        # run subcommand function\n",
    "        res = globals()[\"run_%s\" % args.command](args)\n",
    "    except KeyboardInterrupt:\n",
    "        log_error(\"aborted\")\n",
    "        res = 1\n",
    "    except Exception:\n",
    "        log_internal_error()\n",
    "        res = 2\n",
    "    return res\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    sys.exit(main())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sections: ['spider']\n"
     ]
    }
   ],
   "source": [
    "## 读取配置文件config_load\n",
    "from configparser import ConfigParser\n",
    "\n",
    "cf = ConfigParser()\n",
    " \n",
    "cf.read(\"spider.conf\", encoding='utf-8')\n",
    " \n",
    "secs = cf.sections()\n",
    "print('sections:', secs)\n",
    "\n",
    "\n",
    "#读取配置\n",
    "url_list_file = cf.get('spider', 'url_list_file') # 种子文件路径\n",
    "output_directory = cf.get('spider', 'output_directory') # 抓取结果存储目录\n",
    "max_depth = cf.getint('spider', 'max_depth') # 最大抓取深度(种子为0级)\n",
    "crawl_interval = cf.getint('spider', 'crawl_interval') # 抓取间隔. 单位：秒\n",
    "crawl_timeout = cf.getint('spider', 'crawl_timeout') # 抓取超时. 单位：秒\n",
    "target_url = cf.get('spider', 'target_url') # 需要存储的目标网页URL pattern(正则表达式) ,需要考虑兼容抓取html等情况，不止是抓取图片\n",
    "thread_count = cf.getint('spider', 'thread_count') # 抓取线程数 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://cup.baidu.com/spider/',\n",
       " 'http://www.baidu.com',\n",
       " 'http://www.sina.com.cn']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://cup.baidu.com/spider/', 'http://www.baidu.com', 'http://www.sina.com.cn']\n"
     ]
    }
   ],
   "source": [
    "#去除读取url后的换行符“\\n”\n",
    "with open(url_list_file+'/url_seeds') as f:    \n",
    "    urls = f.readlines()\n",
    "    url_list_processed = []\n",
    "    for i in range(len(urls)):\n",
    "        urls[i] = urls[i].rstrip('\\n')\n",
    "        url_list_processed.append(urls[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\r\n",
      "<head>\r\n",
      "\t<script>\r\n",
      "\t\tlocation.replace(location.href.replace(\"https://\",\"http://\"));\r\n",
      "\t</script>\r\n",
      "</head>\r\n",
      "<body>\r\n",
      "\t<noscript><meta http-equiv=\"refresh\" content=\"0;url=http://www.baidu.com/\"></noscript>\r\n",
      "</body>\r\n",
      "</html>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<html>\\r\\n<head>\\r\\n\\t<script>\\r\\n\\t\\tlocation.replace(location.href.replace(\"https://\",\"http://\"));\\r\\n\\t</script>\\r\\n</head>\\r\\n<body>\\r\\n\\t<noscript><meta http-equiv=\"refresh\" content=\"0;url=http://www.baidu.com/\"></noscript>\\r\\n</body>\\r\\n</html>'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask_url(\"https://www.baidu.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "##爬取网页crawler\n",
    "\n",
    "#解析爬取的内容 \n",
    "def get_data(base_url):\n",
    "    for url in url_list_processed:\n",
    "        html = ask_url(url)\n",
    "\n",
    "\n",
    "#得到指定一个url的网页内容\n",
    "def ask_url(url):\n",
    "    # head = {...}\n",
    "    request = urllib.request.Request(url)\n",
    "    html = \"\"\n",
    "    try:\n",
    "        response = urllib.request.urlopen(request)\n",
    "        html = response.read().decode(\"utf-8\")\n",
    "        print(html)\n",
    "    except urllib.error.URLError as e:\n",
    "        if hasattr(e,\"code\"):\n",
    "            print(e.code)\n",
    "        if hasattr(e,\"reason\"):\n",
    "            print(e.reason)\n",
    "    \n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "unknown url type: 'www.baidu.com'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-6766af0a29a5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'www.baidu.com'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\mlai19\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, url, data, headers, origin_req_host, unverifiable, method)\u001b[0m\n\u001b[0;32m    327\u001b[0m                  \u001b[0morigin_req_host\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munverifiable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                  method=None):\n\u001b[1;32m--> 329\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    330\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheaders\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munredirected_hdrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mlai19\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mfull_url\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m    353\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_full_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_full_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfragment\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplittag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_full_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 355\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeleter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mlai19\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36m_parse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    382\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplittype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_full_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 384\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"unknown url type: %r\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    385\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplithost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: unknown url type: 'www.baidu.com'"
     ]
    }
   ],
   "source": [
    "urllib.request.Request('www.baidu.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##保存数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sections: ['db', 'concurrent']\n",
      "options: ['db_port', 'db_user', 'db_host', 'db_pass']\n",
      "db: [('db_port', '3306'), ('db_user', 'root'), ('db_host', '127.0.0.1'), ('db_pass', 'fxtxy')]\n",
      "db_host: 127.0.0.1\n",
      "db_port: 3306\n",
      "db_user: root\n",
      "db_pass: fxtxy\n",
      "thread: 10\n",
      "processor: 20\n"
     ]
    }
   ],
   "source": [
    "from configparser import ConfigParser\n",
    "\n",
    "cf = ConfigParser()\n",
    " \n",
    "cf.read(\"test.conf\", encoding='utf-8')\n",
    " \n",
    "#return all section\n",
    "secs = cf.sections()\n",
    "print('sections:', secs)\n",
    "\n",
    "opts = cf.options(\"db\")\n",
    "print('options:', opts)\n",
    " \n",
    "kvs = cf.items(\"db\")\n",
    "print('db:', kvs)\n",
    " \n",
    "#read by type\n",
    "db_host = cf.get(\"db\", \"db_host\")\n",
    "db_port = cf.getint(\"db\", \"db_port\")\n",
    "db_user = cf.get(\"db\", \"db_user\")\n",
    "db_pass = cf.get(\"db\", \"db_pass\")\n",
    " \n",
    "#read int\n",
    "threads = cf.getint(\"concurrent\", \"thread\")\n",
    "processors = cf.getint(\"concurrent\", \"processor\")\n",
    " \n",
    "print(\"db_host:\", db_host)\n",
    "print(\"db_port:\", db_port)\n",
    "print(\"db_user:\", db_user)\n",
    "print(\"db_pass:\", db_pass)\n",
    " \n",
    "print(\"thread:\", threads)\n",
    "print(\"processor:\", processors)\n",
    " \n",
    "#modify one value and write to file\n",
    "cf.set(\"db\", \"db_pass\", \"fxtxy\")\n",
    "cf.write(open(\"test.conf\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'root'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xgmtest'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
